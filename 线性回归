#!/usr/bin/env python
# coding: utf-8

# In[1]:


from mxnet import nd
from time import time


# In[2]:


a=nd.ones(shape=1000)
b=nd.ones(shape=1000)


# In[3]:


start=time()
c=nd.zeros(shape=1000)
for i in range(1000):
    c[i]=a[i]+b[i]
time()-start


# In[4]:


start=time()
d=a+b
time()-start


# In[5]:


a=nd.ones(shape=3)
b=10
a+b


# In[6]:


get_ipython().run_line_magic('matplotlib', 'inline')
from IPython import display
from matplotlib import pyplot as plt
from mxnet import autograd,nd
import random


# In[7]:


num_inputs=2
num_examples=1000
true_w=[2,-3.4]
true_b=4.2
features=nd.random.normal(scale=1,shape=(num_examples,num_inputs))
labels=true_w[0]*features[:,0]+true_w[1]*features[:,1]+true_b
labels+=nd.random.normal(scale=0.01,shape=labels.shape)


# In[8]:


features[0],labels[0]


# In[9]:


def use_svg_display():
    # 用矢量图显示
    display.set_matplotlib_formats('svg')
def set_figsize(figsize=(3.5,2.5)):
    use_svg_display()
    # 设置图的尺寸
    plt.rcParams['figure.figsize']=figsize
    
set_figsize()
plt.scatter(features[:,1].asnumpy(),labels.asnumpy(),s=1); # 加分号只显示图


# In[15]:


def data_iter(batch_size,features,labels):
    num_examples=len(features)
    indices=list(range(num_examples))
    random.shuffle(indices) # 样本的读取顺序是随机的
    for i in range(0,num_examples,batch_size):
        j=nd.array(indices[i:min(i+batch_size,num_examples)])
        yield features.take(j),labels.take(j) #take函数根据索引返回对应元素


# In[16]:


batch_size=10

for X,y in data_iter(batch_size,features,labels):
    print(X,y)
    break


# In[20]:


w=nd.random.normal(scale=0.01,shape=(num_inputs,1))
b=nd.zeros(shape=(1,))


# In[21]:


w.attach_grad()
b.attach_grad()


# In[22]:


def linreg(X,w,b): #定义模型
    return nd.dot(X,w)+b


# In[23]:


def squared_loss(y_hat,y):  #定义损失函数
    return(y_hat-y.reshape(y_hat.shape))**2/2


# In[26]:


def sgd(params,lr,batch_size): #定义优化算法
    for param in params:
        param[:]=param-lr*param.grad/batch_size


# In[27]:


lr=0.03
num_epochs=3
net=linreg
loss=squared_loss

for epoch in range(num_epochs): # 训练模型一共需要num_epochs个迭代周期，在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本能够被批量大小整除）。X和y分别是小批量样本的特征和标签
    for X,y in data_iter(batch_size,features,labels):
        with autograd.record():
            l=loss(net(X,w,b),y)  # l是有关小批量X和y的损失
        l.backward()  # 小批量的损失对模型参数求梯度
        sgd([w,b],lr,batch_size) #使用小批量随机梯度下降迭代模型参数
    train_l=loss(net(features,w,b),labels)
    print('epoch %d,loss %f' % (epoch+1,train_l.mean().asnumpy()))
